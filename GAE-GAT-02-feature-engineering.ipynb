{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d144773d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os.path as osp\n",
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import random\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "from pm4py.objects.log.exporter.xes import exporter as xes_exporter\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from pm4py.algo.filtering.log.attributes import attributes_filter\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Dataset, Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import NNConv\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf42b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkinbaronDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super(SkinbaronDataset, self).__init__(root, transform, pre_transform, pre_filter)\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return processed_graphs\n",
    "    def len(self):\n",
    "        return len(self.processed_file_names)\n",
    "    def get(self, idx):\n",
    "        data = torch.load(osp.join(self.processed_dir, f'data_{idx}.pt'))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289d5263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getListofallkeys(dict):      \n",
    "    return list(dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbce907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for getting meta date from the event log\n",
    "def logStatistics (event_log):\n",
    "    # set required variables\n",
    "    categorical_value_list = []\n",
    "    cardinality_list = []\n",
    "    cat_encoder_list = []\n",
    "    max_len = 0\n",
    "    # get list of all activities in event log\n",
    "    act_dict = attributes_filter.get_attribute_values(event_log, \"concept:name\")\n",
    "    activities = list(act_dict.keys())\n",
    "    # one-hot encoding for activity labels\n",
    "    act_array = np.array(activities)\n",
    "    act_enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    act_enc.fit(act_array.reshape(-1, 1))\n",
    "    # get list of all categorical attributes in the log\n",
    "    categorical_attributes = getListofallkeys(event_log[0][0])\n",
    "    categorical_attributes.remove('case_id')\n",
    "    categorical_attributes.remove('concept:name')\n",
    "    # get list of values, and cardinality of each of categorical attributes.\n",
    "    for category in categorical_attributes:\n",
    "        current_cat_dict = attributes_filter.get_attribute_values(event_log, category)\n",
    "        current_cat_list = list(current_cat_dict.keys())\n",
    "        categorical_value_list.append(current_cat_list)\n",
    "        cardinality_list.append(len(current_cat_list))\n",
    "        # one-hot encoding for each of categorical attribute\n",
    "        current_array = np.array(current_cat_list)\n",
    "        current_enc = OneHotEncoder(handle_unknown='ignore')\n",
    "        current_enc.fit(current_array.reshape(-1, 1))\n",
    "        cat_encoder_list.append(current_enc)\n",
    "    # get length of the longest case in the log\n",
    "    num_cases = len(event_log)\n",
    "    for i in range (num_cases):\n",
    "        current_length = len(event_log[i])\n",
    "        max_len = max(max_len, current_length)\n",
    "    return num_cases, max_len, act_array, act_enc, categorical_attributes, cardinality_list, cat_encoder_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e1d8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method for creating node feature matrix for one trace, additionally we create a m_array dict, and pair_dict!\n",
    "def build_node_feature_matrix(current_trace, activity_label_array, act_encoder, max_trace_length):\n",
    "    # Compute length of trace\n",
    "    trace_length = len(current_trace)\n",
    "    # dimensions of one-hot encoder for activity labels:a dim*dim array:\n",
    "    dim = len(activity_label_array)\n",
    "    \"\"\" initialize a dictionary to collect:\n",
    "        Keys: tuple of activity labels (directly-followed pairs) \n",
    "        values: position of pair's occurrence in the trace\n",
    "    \"\"\"\n",
    "    pair_dict = {}\n",
    "    # initialize an auxiliary list to collect to collect unique activity labels in the current case\n",
    "    act_list = []\n",
    "    \"\"\" initialize a dictionary to collect:\n",
    "        Keys: tuple of source node index, and target node index\n",
    "        values: relevant m-array\n",
    "    \"\"\"\n",
    "    m_array_dict = {}\n",
    "    # creating pair_dict dictionary\n",
    "    for i in range(trace_length - 1):\n",
    "        current_activity_label = current_trace[i].get('concept:name')\n",
    "        act_list.append(current_activity_label)\n",
    "        next_activity_label = current_trace[i + 1].get('concept:name')\n",
    "        act_list.append(next_activity_label)\n",
    "        activity_tuple = (current_activity_label, next_activity_label)\n",
    "        if activity_tuple in pair_dict:\n",
    "            pair_dict[activity_tuple].append(i)\n",
    "        else:\n",
    "            pair_dict[activity_tuple] = [i]\n",
    "    # identify unique activity labels, and initialize node feature matrix based on its size.\n",
    "    act_set = list(set(act_list))\n",
    "    node_feature = np.zeros((len(act_set), dim), dtype=np.float32)\n",
    "    # create node feature matrix based on one-hot encoding\n",
    "    counter_act = 0\n",
    "    for act in act_set:\n",
    "        act_arr = np.array(act)\n",
    "        node_feature[counter_act] = act_encoder.transform(act_arr.reshape(-1, 1)).toarray()\n",
    "        counter_act += 1\n",
    "    # iterate over all directly-followed pairs, identify their source and target index in the node set.\n",
    "    # create m-array based on the position(s) of their occurrence, and return indexes, and array in a dictionary\n",
    "    for key in pair_dict:\n",
    "        source_index = act_set.index(key[0])\n",
    "        target_index = act_set.index(key[1])\n",
    "        m_array = np.zeros(max_trace_length - 1, dtype=np.intc)\n",
    "        for i in range(len(pair_dict[key])):\n",
    "            m_array[pair_dict[key][i]] = 1\n",
    "        m_array_dict[(source_index, target_index)] = m_array\n",
    "    return pair_dict, node_feature, m_array_dict, act_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4b7a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method for creating p_array dict, its structure is similar to the structure of m_array dict!\n",
    "# keys are exactly the same,\n",
    "def build_p_array_dict(current_trace, m_array, pairs, act_set, max_trace_length, categorical, cardinality, cat_encoder_list):\n",
    "    p_array_dict = {}\n",
    "    m_size = sum(cardinality) # no need for extra dimension for timestamp as well as binary, numerical, ordinal atts\n",
    "    for key in m_array:\n",
    "        p_array = np.zeros((max_trace_length - 1) * m_size, dtype=np.float32)\n",
    "        source_index = key[0]\n",
    "        target_index = key[1]\n",
    "        source_act = act_set[source_index]\n",
    "        target_act = act_set[target_index]\n",
    "        act_tuple = (source_act, target_act)\n",
    "        current_occurrence = pairs[act_tuple]  # is a list\n",
    "        for i in current_occurrence:\n",
    "            # get the changes for categorical attributes\n",
    "            pointer = 0\n",
    "            iterator = 0\n",
    "            for j in categorical:\n",
    "                curr_card = cardinality[iterator]\n",
    "                source_att = current_trace[i].get(j)\n",
    "                target_att = current_trace[i + 1].get(j)\n",
    "                if str(target_att) != 'nan':\n",
    "                    curr_arr = np.array(target_att)\n",
    "                    target_onehot = cat_encoder_list[iterator].transform(curr_arr.reshape(-1, 1)).toarray()\n",
    "                    if str(source_att) == 'nan':\n",
    "                        p_array[i * m_size + pointer: i * m_size + pointer + curr_card] = target_onehot\n",
    "                    else:\n",
    "                        if target_att != source_att:\n",
    "                            curr_arr2 = np.array(source_att)\n",
    "                            source_onehot = cat_encoder_list[iterator].transform(curr_arr2.reshape(-1, 1)).toarray()\n",
    "                            p_array[i * m_size + pointer: i * m_size + pointer + curr_card] = target_onehot - source_onehot\n",
    "                iterator = iterator + 1\n",
    "                pointer = pointer + curr_card\n",
    "                # now that we have the value (p_array), we link it to the current key in our dictionary\n",
    "        p_array_dict[key] = p_array\n",
    "    return p_array_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077abfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "interest_path = r'D:\\Final master thesis evaluation\\exp1'\n",
    "log_pattern = r'D:\\Final master thesis evaluation\\exp1\\*.xes'\n",
    "processed_path = r'D:\\Final master thesis evaluation\\exp1\\data\\processed'\n",
    "case_id_target = r'D:\\Final master thesis evaluation\\exp1\\case_id_list.pkl'\n",
    "processed_Pattern = r\"D:\\Final master thesis evaluation\\exp1\\data\\processed\\*.pt\"\n",
    "targat_path = r\"D:\\Final master thesis evaluation\\exp1\\data\"\n",
    "logs = glob.glob(log_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29c3536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the log\n",
    "event_log = xes_importer.apply(logs[0])\n",
    "# get required meta data\n",
    "num_cases, tmax, act_array, act_enc, categorical_attributes, cardinality_list, cat_encoder_list = logStatistics(event_log)\n",
    "# core part of feature eng.\n",
    "case_id_list = []\n",
    "idx = 0\n",
    "for i in range (len(event_log)):\n",
    "    case = event_log [i]\n",
    "    pairs, node_feature_matrix, m_array_dict, activity_set = build_node_feature_matrix(case, act_array, act_enc, tmax)\n",
    "    p_array_dict = build_p_array_dict(case, m_array_dict, pairs, activity_set, tmax, categorical_attributes,\n",
    "                                          cardinality_list, cat_encoder_list)\n",
    "    case_id_list.append(case.attributes.get('concept:name'))\n",
    "    edge_index_list = []\n",
    "    edge_att_list = []\n",
    "    edge_marray_list = []\n",
    "    for key in p_array_dict:\n",
    "        edge_index_list.append(list(key))\n",
    "        edge_att_list.append(list(p_array_dict.get(key)))\n",
    "        edge_marray_list.append(list(m_array_dict.get(key)))\n",
    "    edge_index = torch.tensor(edge_index_list, dtype=torch.long)\n",
    "    edge_attr = torch.tensor(edge_att_list, dtype=torch.float)\n",
    "    edge_m_array = torch.tensor(edge_marray_list, dtype=torch.uint8)               \n",
    "    x = torch.from_numpy(node_feature_matrix).float()\n",
    "    graph = Data(x=x, edge_index=edge_index.t().contiguous(), edge_attr=edge_attr, edge_m_array=edge_m_array)\n",
    "    torch.save(graph, osp.join(processed_path, f'data_{idx}.pt'))\n",
    "    idx += 1\n",
    "case_id_file = open(case_id_target, \"wb\")\n",
    "pickle.dump(case_id_list, case_id_file)\n",
    "case_id_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
