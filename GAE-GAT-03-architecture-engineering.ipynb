{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6569b5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os.path as osp\n",
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import random\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "from pm4py.objects.log.exporter.xes import exporter as xes_exporter\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from pm4py.algo.filtering.log.attributes import attributes_filter\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Dataset, Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import NNConv\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ad7d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nextplot(force=False):\n",
    "    \"\"\"Start a new plot.\n",
    "\n",
    "    In a notebook (or if `force=True`), create a new figure. Otherwise (e.g, in\n",
    "    IPython), clear the current figure.\n",
    "\n",
    "    \"\"\"\n",
    "    inNotebook = \"IPKernelApp\" in get_ipython().config\n",
    "    if force or inNotebook:\n",
    "        plt.figure()  # this creates a new plot\n",
    "    else:\n",
    "        plt.clf()  # and this clears the current one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89a6a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkinbaronDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super(SkinbaronDataset, self).__init__(root, transform, pre_transform, pre_filter)\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return processed_graphs\n",
    "    def len(self):\n",
    "        return len(self.processed_file_names)\n",
    "    def get(self, idx):\n",
    "        data = torch.load(osp.join(self.processed_dir, f'data_{idx}.pt'))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f31a119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define GCN (ECC), and GAT neural network architectures\n",
    "class ECCCONV(nn.Module):\n",
    "    \"\"\"Define ECCCONV class for the Edge-Conditioned Convolutional (ECC) neural network.\"\"\"\n",
    "\n",
    "    def __init__(self, size_in, size_out, edge_size, tmax):\n",
    "        super(ECCCONV, self).__init__()\n",
    "        nn1 = nn.Linear(edge_size, size_in * size_out)\n",
    "        self.conv1 = NNConv(size_in, size_out, nn1)\n",
    "        self.lin1 = nn.Linear(size_in, size_out, bias=False)\n",
    "        self.readout = nn.Linear(2 * size_out, tmax - 1)\n",
    "\n",
    "    def forward(self, graph):\n",
    "        graph.x = F.elu(self.conv1(graph.x, graph.edge_index, graph.edge_attr) + self.lin1(graph.x))\n",
    "        s_t_nodes = torch.cat((graph.x[int(graph.edge_index[0, 0])], graph.x[int(graph.edge_index[1, 0])]))\n",
    "        m_array_pred = torch.sigmoid(self.readout(s_t_nodes))\n",
    "        for i in range(1, graph.edge_index.shape[1]):  # iterate over the number of edges in the batch (called graph)\n",
    "            s_t_nodes = torch.cat((graph.x[int(graph.edge_index[0, i])], graph.x[int(graph.edge_index[1, i])]))\n",
    "            edge_label_pred = torch.sigmoid(self.readout(s_t_nodes))\n",
    "            m_array_pred = torch.cat((m_array_pred, edge_label_pred), 0)\n",
    "        m_array_pred = m_array_pred.view(-1, tmax - 1)\n",
    "        return m_array_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71616665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define architecture of GAE\n",
    "class GATL1noSelf(nn.Module):\n",
    "    \"\"\"Define GATL1noSelf class for the Graph Attention neural network.(GAT)\n",
    "    with one layer, and without self loops\"\"\"\n",
    "\n",
    "    def __init__(self, size_in, size_out, edge_size, num_head, tmax, dropout_prob):\n",
    "        super(GATL1noSelf, self).__init__()\n",
    "        self.conv1 = GATv2Conv(in_channels=size_in, out_channels=size_out, heads=num_head, dropout=dropout_prob,\n",
    "                               add_self_loops=False, edge_dim=edge_size)\n",
    "        self.readout = nn.Linear(2 * num_head * size_out, tmax - 1)\n",
    "\n",
    "    def forward(self, graph):\n",
    "        graph.x = F.elu(self.conv1(graph.x, graph.edge_index, graph.edge_attr))\n",
    "        s_t_nodes = torch.cat((graph.x[int(graph.edge_index[0, 0])], graph.x[int(graph.edge_index[1, 0])]))\n",
    "        m_array_pred = torch.sigmoid(self.readout(s_t_nodes))\n",
    "        for i in range(1, graph.edge_index.shape[1]):  # iterate over the number of edges in the batch (called graph)\n",
    "            s_t_nodes = torch.cat((graph.x[int(graph.edge_index[0, i])], graph.x[int(graph.edge_index[1, i])]))\n",
    "            edge_label_pred = torch.sigmoid(self.readout(s_t_nodes))\n",
    "            m_array_pred = torch.cat((m_array_pred, edge_label_pred), 0)\n",
    "        m_array_pred = m_array_pred.view(-1, tmax - 1)\n",
    "        return m_array_pred\n",
    "\n",
    "class GATL1withSelf(nn.Module):\n",
    "    \"\"\"Define GATL1withSelf class for the Graph Attention neural network.(GAT)\n",
    "    with one layer, and self loops\"\"\"\n",
    "\n",
    "    def __init__(self, size_in, size_out, edge_size, num_head, tmax, dropout_prob):\n",
    "        super(GATL1withSelf, self).__init__()\n",
    "        self.conv1 = GATv2Conv(in_channels=size_in, out_channels=size_out, heads=num_head, edge_dim=edge_size,\n",
    "                               dropout=dropout_prob, fill_value='mean')\n",
    "        self.readout = nn.Linear(2 * num_head * size_out, tmax - 1)\n",
    "\n",
    "    def forward(self, graph):\n",
    "        graph.x = F.elu(self.conv1(graph.x, graph.edge_index, graph.edge_attr))\n",
    "        s_t_nodes = torch.cat((graph.x[int(graph.edge_index[0, 0])], graph.x[int(graph.edge_index[1, 0])]))\n",
    "        m_array_pred = torch.sigmoid(self.readout(s_t_nodes))\n",
    "        for i in range(1, graph.edge_index.shape[1]):  # iterate over the number of edges in the batch (called graph)\n",
    "            s_t_nodes = torch.cat((graph.x[int(graph.edge_index[0, i])], graph.x[int(graph.edge_index[1, i])]))\n",
    "            edge_label_pred = torch.sigmoid(self.readout(s_t_nodes))\n",
    "            m_array_pred = torch.cat((m_array_pred, edge_label_pred), 0)\n",
    "        m_array_pred = m_array_pred.view(-1, tmax - 1)\n",
    "        return m_array_pred\n",
    "    \n",
    "\n",
    "class GATL2noSelf(nn.Module):\n",
    "    \"\"\"Define GATL2noSelf class for the Graph Attention neural network.(GAT)\n",
    "    with two layers, and without self loops\"\"\"\n",
    "\n",
    "    def __init__(self, size_in, size_out, size_hid1, edge_size, num_head, tmax, dropout_prob):\n",
    "        super(GATL2noSelf, self).__init__()\n",
    "        self.conv1 = GATv2Conv(in_channels=size_in, out_channels=size_hid1, heads=num_head, dropout=dropout_prob,\n",
    "                               add_self_loops=False, edge_dim=edge_size)\n",
    "        self.conv2 = GATv2Conv(in_channels=num_head * size_hid1, out_channels=size_out, heads=num_head, dropout=dropout_prob,\n",
    "                               add_self_loops=False, edge_dim=edge_size)\n",
    "        self.readout = nn.Linear(2 * num_head * size_out, tmax - 1)\n",
    "\n",
    "    def forward(self, graph):\n",
    "        graph.x = F.elu(self.conv1(graph.x, graph.edge_index, graph.edge_attr))\n",
    "        graph.x = F.elu(self.conv2(graph.x, graph.edge_index, graph.edge_attr))\n",
    "        s_t_nodes = torch.cat((graph.x[int(graph.edge_index[0, 0])], graph.x[int(graph.edge_index[1, 0])]))\n",
    "        m_array_pred = torch.sigmoid(self.readout(s_t_nodes))\n",
    "        for i in range(1, graph.edge_index.shape[1]):  # iterate over the number of edges in the batch (called graph)\n",
    "            s_t_nodes = torch.cat((graph.x[int(graph.edge_index[0, i])], graph.x[int(graph.edge_index[1, i])]))\n",
    "            edge_label_pred = torch.sigmoid(self.readout(s_t_nodes))\n",
    "            m_array_pred = torch.cat((m_array_pred, edge_label_pred), 0)\n",
    "        m_array_pred = m_array_pred.view(-1, tmax - 1)\n",
    "        return m_array_pred\n",
    "    \n",
    "class GATL2withSelf(nn.Module):\n",
    "    \"\"\"Define GATL2withSelf class for the Graph Attention neural network.(GAT)\n",
    "    with two layers, and self loops\"\"\"\n",
    "\n",
    "    def __init__(self, size_in, size_out, size_hid1, edge_size, num_head, tmax, dropout_prob):\n",
    "        super(GATL2withSelf, self).__init__()\n",
    "        self.conv1 = GATv2Conv(in_channels=size_in, out_channels=size_hid1, heads=num_head, edge_dim=edge_size,\n",
    "                               dropout=dropout_prob, fill_value='mean')\n",
    "        self.conv2 = GATv2Conv(in_channels=num_head * size_hid1, out_channels=size_out, heads=num_head, edge_dim=edge_size,\n",
    "                               dropout=dropout_prob, fill_value='mean')\n",
    "        self.readout = nn.Linear(2 * num_head * size_out, tmax - 1)\n",
    "\n",
    "    def forward(self, graph):\n",
    "        graph.x = F.elu(self.conv1(graph.x, graph.edge_index, graph.edge_attr))\n",
    "        graph.x = F.elu(self.conv2(graph.x, graph.edge_index, graph.edge_attr))\n",
    "        s_t_nodes = torch.cat((graph.x[int(graph.edge_index[0, 0])], graph.x[int(graph.edge_index[1, 0])]))\n",
    "        m_array_pred = torch.sigmoid(self.readout(s_t_nodes))\n",
    "        for i in range(1, graph.edge_index.shape[1]):  # iterate over the number of edges in the batch (called graph)\n",
    "            s_t_nodes = torch.cat((graph.x[int(graph.edge_index[0, i])], graph.x[int(graph.edge_index[1, i])]))\n",
    "            edge_label_pred = torch.sigmoid(self.readout(s_t_nodes))\n",
    "            m_array_pred = torch.cat((m_array_pred, edge_label_pred), 0)\n",
    "        m_array_pred = m_array_pred.view(-1, tmax - 1)\n",
    "        return m_array_pred\n",
    "    \n",
    "class GATL3noSelf(nn.Module):\n",
    "    \"\"\"Define GATL3noSelf class for the Graph Attention neural network.(GAT)\n",
    "    with three layers, and without self loops\"\"\"\n",
    "\n",
    "    def __init__(self, size_in, size_out, size_hid1, size_hid2, edge_size, num_head, tmax, dropout_prob):\n",
    "        super(GATL3noSelf, self).__init__()\n",
    "        self.conv1 = GATv2Conv(in_channels=size_in, out_channels=size_hid1, heads=num_head, dropout=dropout_prob,\n",
    "                               add_self_loops=False, edge_dim=edge_size)\n",
    "        self.conv2 = GATv2Conv(in_channels=num_head * size_hid1, out_channels=size_hid2, heads=num_head, dropout=dropout_prob,\n",
    "                               add_self_loops=False, edge_dim=edge_size)\n",
    "        self.conv3 = GATv2Conv(in_channels=num_head * size_hid2, out_channels=size_out, heads=num_head, dropout=dropout_prob,\n",
    "                               add_self_loops=False, edge_dim=edge_size)\n",
    "        self.readout = nn.Linear(2 * num_head * size_out, tmax - 1)\n",
    "\n",
    "    def forward(self, graph):\n",
    "        graph.x = F.elu(self.conv1(graph.x, graph.edge_index, graph.edge_attr))\n",
    "        graph.x = F.elu(self.conv2(graph.x, graph.edge_index, graph.edge_attr))\n",
    "        graph.x = F.elu(self.conv3(graph.x, graph.edge_index, graph.edge_attr))\n",
    "        s_t_nodes = torch.cat((graph.x[int(graph.edge_index[0, 0])], graph.x[int(graph.edge_index[1, 0])]))\n",
    "        m_array_pred = torch.sigmoid(self.readout(s_t_nodes))\n",
    "        for i in range(1, graph.edge_index.shape[1]):  # iterate over the number of edges in the batch (called graph)\n",
    "            s_t_nodes = torch.cat((graph.x[int(graph.edge_index[0, i])], graph.x[int(graph.edge_index[1, i])]))\n",
    "            edge_label_pred = torch.sigmoid(self.readout(s_t_nodes))\n",
    "            m_array_pred = torch.cat((m_array_pred, edge_label_pred), 0)\n",
    "        m_array_pred = m_array_pred.view(-1, tmax - 1)\n",
    "        return m_array_pred\n",
    "    \n",
    "class GATL3withSelf(nn.Module):\n",
    "    \"\"\"Define GATL3withSelf class for the Graph Attention neural network.(GAT)\n",
    "    with three layers, and without self loops\"\"\"\n",
    "\n",
    "    def __init__(self, size_in, size_out, size_hid1, size_hid2, edge_size, num_head, tmax, dropout_prob):\n",
    "        super(GATL3withSelf, self).__init__()\n",
    "        self.conv1 = GATv2Conv(in_channels=size_in, out_channels=size_hid1, heads=num_head, edge_dim=edge_size,\n",
    "                               dropout=dropout_prob, fill_value='mean')\n",
    "        self.conv2 = GATv2Conv(in_channels=num_head * size_hid1, out_channels=size_hid2, heads=num_head, edge_dim=edge_size,\n",
    "                               dropout=dropout_prob, fill_value='mean')\n",
    "        self.conv3 = GATv2Conv(in_channels=num_head * size_hid2, out_channels=size_out, heads=num_head, edge_dim=edge_size,\n",
    "                               dropout=dropout_prob, fill_value='mean')\n",
    "        self.readout = nn.Linear(2 * num_head * size_out, tmax - 1)\n",
    "\n",
    "    def forward(self, graph):\n",
    "        graph.x = F.elu(self.conv1(graph.x, graph.edge_index, graph.edge_attr))\n",
    "        graph.x = F.elu(self.conv2(graph.x, graph.edge_index, graph.edge_attr))\n",
    "        graph.x = F.elu(self.conv3(graph.x, graph.edge_index, graph.edge_attr))\n",
    "        s_t_nodes = torch.cat((graph.x[int(graph.edge_index[0, 0])], graph.x[int(graph.edge_index[1, 0])]))\n",
    "        m_array_pred = torch.sigmoid(self.readout(s_t_nodes))\n",
    "        for i in range(1, graph.edge_index.shape[1]):  # iterate over the number of edges in the batch (called graph)\n",
    "            s_t_nodes = torch.cat((graph.x[int(graph.edge_index[0, i])], graph.x[int(graph.edge_index[1, i])]))\n",
    "            edge_label_pred = torch.sigmoid(self.readout(s_t_nodes))\n",
    "            m_array_pred = torch.cat((m_array_pred, edge_label_pred), 0)\n",
    "        m_array_pred = m_array_pred.view(-1, tmax - 1)\n",
    "        return m_array_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56006fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the device to GPU or CPU (only if GPU is not available we will use CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# for reproducibility of the result (note: we use SGD for optimization)\n",
    "np.random.seed(0)\n",
    "# Define binary cross entropy loss.\n",
    "batch_loss = nn.BCELoss()\n",
    "#batch_loss = nn.MSELoss() # in case we use sum of time and control-flow in m-array\n",
    "targat_path = r\"D:\\Final master thesis evaluation\\exp1\\data\"\n",
    "processed_Pattern = r\"D:\\Final master thesis evaluation\\exp1\\data\\processed\\*.pt\"\n",
    "case_id_target = r'D:\\Final master thesis evaluation\\exp1\\case_id_list.pkl'\n",
    "models_target = ['D:\\Final master thesis evaluation\\exp1\\models\\GATl1h1nl_param.pt',\n",
    "                 'D:\\Final master thesis evaluation\\exp1\\models\\GATL1h4nl_param.pt',\n",
    "                 'D:\\Final master thesis evaluation\\exp1\\models\\GATL2h1nl_param.pt',\n",
    "                 'D:\\Final master thesis evaluation\\exp1\\models\\GATL2h4nl_param.pt',\n",
    "                 'D:\\Final master thesis evaluation\\exp1\\models\\GATL3h1nl_param.pt',\n",
    "                 'D:\\Final master thesis evaluation\\exp1\\models\\GATL3h4nl_param.pt',\n",
    "                 'D:\\Final master thesis evaluation\\exp1\\models\\GATL1h1wl_param.pt',\n",
    "                 'D:\\Final master thesis evaluation\\exp1\\models\\GATL1h4wl_param.pt',\n",
    "                 'D:\\Final master thesis evaluation\\exp1\\models\\GATL2h1wl_param.pt',\n",
    "                 'D:\\Final master thesis evaluation\\exp1\\models\\GATL2h4wl_param.pt',\n",
    "                 'D:\\Final master thesis evaluation\\exp1\\models\\GATL3h1wl_param.pt',\n",
    "                 'D:\\Final master thesis evaluation\\exp1\\models\\GATL3h4wl_param.pt']\n",
    "loss_df_target = ['D:\\Final master thesis evaluation\\exp1\\models\\GATl1h1nl_loss.pt',\n",
    "                 'D:\\Final master thesis evaluation\\exp1\\models\\GATL1h4nl_loss.pt',\n",
    "                 'D:\\Final master thesis evaluation\\exp1\\models\\GATL2h1nl_loss.pt',\n",
    "                 'D:\\Final master thesis evaluation\\exp1\\models\\GATL2h4nl_loss.pt',\n",
    "                 'D:\\Final master thesis evaluation\\exp1\\models\\GATL3h1nl_loss.pt',\n",
    "                 'D:\\Final master thesis evaluation\\exp1\\models\\GATL3h4nl_loss.pt',\n",
    "                 'D:\\Final master thesis evaluation\\exp1\\models\\GATL1h1wl_loss.pt',\n",
    "                 'D:\\Final master thesis evaluation\\exp1\\models\\GATL1h4wl_loss.pt',\n",
    "                 'D:\\Final master thesis evaluation\\exp1\\models\\GATL2h1wl_loss.pt',\n",
    "                 'D:\\Final master thesis evaluation\\exp1\\models\\GATL2h4wl_loss.pt',\n",
    "                 'D:\\Final master thesis evaluation\\exp1\\models\\GATL3h1wl_loss.pt',\n",
    "                 'D:\\Final master thesis evaluation\\exp1\\models\\GATL3h4wl_loss.pt']\n",
    "models_txt_target = ['D:\\Final master thesis evaluation\\exp1\\models\\GATl1h1nl.txt',\n",
    "                 'D:\\Final master thesis evaluation\\exp1\\models\\GATL1h4nl.txt',\n",
    "                 'D:\\Final master thesis evaluation\\exp1\\models\\GATL2h1nl.txt',\n",
    "                 'D:\\Final master thesis evaluation\\exp1\\models\\GATL2h4nl.txt',\n",
    "                 'D:\\Final master thesis evaluation\\exp1\\models\\GATL3h1nl.txt',\n",
    "                 'D:\\Final master thesis evaluation\\exp1\\models\\GATL3h4nl.txt',\n",
    "                 'D:\\Final master thesis evaluation\\exp1\\models\\GATL1h1wl.txt',\n",
    "                 'D:\\Final master thesis evaluation\\exp1\\models\\GATL1h4wl.txt',\n",
    "                 'D:\\Final master thesis evaluation\\exp1\\models\\GATL2h1wl.txt',\n",
    "                 'D:\\Final master thesis evaluation\\exp1\\models\\GATL2h4wl.txt',\n",
    "                 'D:\\Final master thesis evaluation\\exp1\\models\\GATL3h1wl.txt',\n",
    "                 'D:\\Final master thesis evaluation\\exp1\\models\\GATL3h4wl.txt']\n",
    "# creating dataset\n",
    "processed_graphs = glob.glob(processed_Pattern)\n",
    "dataset = SkinbaronDataset(root=targat_path)\n",
    "# train/validation/test split\n",
    "train_dataset = dataset[0:9000]\n",
    "validation_dataset = dataset[9000:12000]\n",
    "test_dataset = dataset[12000:15000]\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of validation graphs: {len(validation_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')\n",
    "# Retrieving the case_id list from saved file on the disk:\n",
    "case_id_file = open(case_id_target, \"rb\")\n",
    "case_id_list = pickle.load(case_id_file)\n",
    "case_id_file.close()\n",
    "# find the length of the longest trace based on the edge_m_array attribute\n",
    "tmax = int(dataset.get(0).edge_m_array.shape[1]) + 1\n",
    "dropout_prob = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f79486",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_heads = [1,4,1,4,1,4,1,4,1,4,1,4]\n",
    "for i in range (12):\n",
    "    # initialize the model\n",
    "    num_head = number_heads[i]\n",
    "    size_in = dataset.get(0).x.shape[1]\n",
    "    size_out = int(2 * dataset.get(0).x.shape[1] / num_head)       \n",
    "    edge_size = dataset.get(0).edge_attr.shape[1]    \n",
    "    if i == 0 or i == 1:\n",
    "        model = GATL1noSelf(size_in, size_out, edge_size, num_head, tmax, dropout_prob)\n",
    "    elif i == 2 or i == 3:\n",
    "        size_hid1 = int(1.5 * dataset.get(0).x.shape[1] / num_head)\n",
    "        model = GATL2noSelf(size_in, size_out, size_hid1, edge_size, num_head, tmax, dropout_prob)   \n",
    "    elif i == 4 or i == 5:\n",
    "        size_hid1 = int(1.33 * dataset.get(0).x.shape[1] / num_head)\n",
    "        size_hid2 = int(1.67 * dataset.get(0).x.shape[1] / num_head)\n",
    "        model = GATL3noSelf(size_in, size_out, size_hid1, size_hid2, edge_size, num_head, tmax, dropout_prob)\n",
    "    elif i == 6 or i == 7:\n",
    "        model = GATL1withSelf(size_in, size_out, edge_size, num_head, tmax, dropout_prob)\n",
    "    elif i == 8 or i == 9:\n",
    "        size_hid1 = int(1.5 * dataset.get(0).x.shape[1] / num_head)\n",
    "        model = GATL2withSelf(size_in, size_out, size_hid1, edge_size, num_head, tmax, dropout_prob)\n",
    "    elif i == 10 or i == 11:\n",
    "        size_hid1 = int(1.33 * dataset.get(0).x.shape[1] / num_head)\n",
    "        size_hid2 = int(1.67 * dataset.get(0).x.shape[1] / num_head)\n",
    "        model = GATL3withSelf(size_in, size_out, size_hid1, size_hid2, edge_size, num_head, tmax, dropout_prob)    \n",
    "    # setting the minibatch size\n",
    "    batch_size = 64\n",
    "    loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "    # move to GPU (if available)\n",
    "    model = model.to(device)\n",
    "    # inizialize the optimizer\n",
    "    lr = 0.01\n",
    "    weight_decay = 5e-4\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    with open(models_txt_target[i], \"w\") as GAT2_file:\n",
    "        # Print model's state_dict\n",
    "        print(\"Model's state_dict:\", file=GAT2_file)\n",
    "        for param_tensor in model.state_dict():\n",
    "            print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size(), file=GAT2_file)\n",
    "        # Print optimizer's state_dict\n",
    "        print(\"Optimizer's state_dict:\", file=GAT2_file)\n",
    "        for var_name in optimizer.state_dict():\n",
    "            print(var_name, \"\\t\", optimizer.state_dict()[var_name], file=GAT2_file)\n",
    "        GAT2_file.close()\n",
    "    # training of the model\n",
    "    epochs = 100\n",
    "    train_hist = {}\n",
    "    train_hist['loss'] = []\n",
    "    # Initialize training\n",
    "    for layer in model.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            layer.reset_parameters()\n",
    "    model.train()\n",
    "    # training loop\n",
    "    begin_time = datetime.now()\n",
    "    for epoch in range(epochs):\n",
    "        print(epoch)\n",
    "        for idx, data_batch in enumerate(loader):\n",
    "            data_batch = data_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = batch_loss(model.forward(data_batch).float(), data_batch.edge_m_array.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_hist['loss'].append(loss.item())\n",
    "            with open(models_txt_target[i], \"a\") as GAT2_file:\n",
    "                print('[Epoch %4d/%4d] [Batch %4d/%4d] Loss: % 2.2e' % (epoch + 1, epochs,idx + 1, len(loader),\n",
    "                                                                        loss.item()), file=GAT2_file)\n",
    "                GAT2_file.close()\n",
    "    finish_time = datetime.now()\n",
    "    with open(models_txt_target[i], \"a\") as GAT2_file:\n",
    "        print('Training time:', file=GAT2_file)\n",
    "        print(finish_time - begin_time, file=GAT2_file)\n",
    "        GAT2_file.close()    \n",
    "    # save and load learned parameters\n",
    "    torch.save(model.state_dict(), models_target[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0089f05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get anomaly scores produced on validation dataset\n",
    "number_heads = [1,4,1,4,1,4,1,4,1,4,1,4]\n",
    "for i in range (12):\n",
    "    # initialize the model\n",
    "    num_head = number_heads[i]\n",
    "    size_in = dataset.get(0).x.shape[1]\n",
    "    size_out = int(2 * dataset.get(0).x.shape[1] / num_head)       \n",
    "    edge_size = dataset.get(0).edge_attr.shape[1]    \n",
    "    if i == 0 or i == 1:\n",
    "        model = GATL1noSelf(size_in, size_out, edge_size, num_head, tmax, dropout_prob)\n",
    "    elif i == 2 or i == 3:\n",
    "        size_hid1 = int(1.5 * dataset.get(0).x.shape[1] / num_head)\n",
    "        model = GATL2noSelf(size_in, size_out, size_hid1, edge_size, num_head, tmax, dropout_prob)   \n",
    "    elif i == 4 or i == 5:\n",
    "        size_hid1 = int(1.33 * dataset.get(0).x.shape[1] / num_head)\n",
    "        size_hid2 = int(1.67 * dataset.get(0).x.shape[1] / num_head)\n",
    "        model = GATL3noSelf(size_in, size_out, size_hid1, size_hid2, edge_size, num_head, tmax, dropout_prob)\n",
    "    elif i == 6 or i == 7:\n",
    "        model = GATL1withSelf(size_in, size_out, edge_size, num_head, tmax, dropout_prob)\n",
    "    elif i == 8 or i == 9:\n",
    "        size_hid1 = int(1.5 * dataset.get(0).x.shape[1] / num_head)\n",
    "        model = GATL2withSelf(size_in, size_out, size_hid1, edge_size, num_head, tmax, dropout_prob)\n",
    "    elif i == 10 or i == 11:\n",
    "        size_hid1 = int(1.33 * dataset.get(0).x.shape[1] / num_head)\n",
    "        size_hid2 = int(1.67 * dataset.get(0).x.shape[1] / num_head)\n",
    "        model = GATL3withSelf(size_in, size_out, size_hid1, size_hid2, edge_size, num_head, tmax, dropout_prob)    \n",
    "    model.load_state_dict(torch.load(models_target[i]))\n",
    "    # get the result in a dataframe\n",
    "    batch_size = 1\n",
    "    loader = DataLoader(validation_dataset, batch_size=batch_size)\n",
    "    validation_id_list = case_id_list[9000:12000]\n",
    "    loss_list = []\n",
    "    begin_time = datetime.now()\n",
    "    for data_batch in loader:\n",
    "        loss = batch_loss(model.forward(data_batch).float(), data_batch.edge_m_array.float())\n",
    "        if loss > 0:\n",
    "            loss_list.append(loss.item())\n",
    "        else:\n",
    "            loss_list.append(0)      \n",
    "    finish_time = datetime.now()    \n",
    "    with open(models_txt_target[i], \"a\") as GAT2_file:\n",
    "        print('Anomaly score computation time for', len(validation_dataset), 'cases:', file=GAT2_file)\n",
    "        print(finish_time - begin_time, file=GAT2_file)\n",
    "        GAT2_file.close()\n",
    "    GAT2_dictionary = {'case_id': validation_id_list, 'Loss': loss_list}\n",
    "    loss_df = pd.DataFrame(GAT2_dictionary)\n",
    "    loss_df.to_pickle(loss_df_target[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcb8e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import label dataframe\n",
    "label_csv = pd.read_csv(r'D:\\Final master thesis evaluation\\exp1\\large-0.1-1.csv')\n",
    "label_csv.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "label_csv[\"case_id\"] = pd.to_numeric(label_csv[\"case_id\"])\n",
    "label_csv.case_id.astype(str).astype(int)\n",
    "label_csv.label.astype(str)\n",
    "#label_csv.dtypes\n",
    "validation_csv = label_csv.loc[label_csv['case_id'] > 9000]\n",
    "validation_csv = validation_csv.loc[label_csv['case_id'] < 12001]\n",
    "#print(len(validation_csv))\n",
    "label_csv1 = validation_csv.loc[validation_csv['label']== 'normal']\n",
    "normal_class = len(label_csv1)\n",
    "anom_class = 3000 - len(label_csv1)\n",
    "print('numnber of anomalies in the log:', anom_class)\n",
    "label_csv.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ecf046",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (12):\n",
    "    loss_df = pd.read_pickle(loss_df_target[i])\n",
    "    loss_df[\"case_id\"] = pd.to_numeric(loss_df[\"case_id\"])\n",
    "    result = pd.merge(loss_df, validation_csv, on=[\"case_id\"])\n",
    "    sorted_result = result.sort_values(by=['Loss'], ascending = False, ignore_index = True)\n",
    "    sorted_list = sorted_result['label'].tolist()\n",
    "    predictions = []\n",
    "    for j in range (len(sorted_list)):\n",
    "        if sorted_list[j] == 'normal':\n",
    "            predictions.append(0)\n",
    "        else:\n",
    "            predictions.append(1)\n",
    "    prediction_array = np.array(predictions)\n",
    "    best_score = 0\n",
    "    for j in range (len(sorted_list)):\n",
    "        current_alarms = prediction_array[0:j+1]\n",
    "        current_normals = prediction_array[j+1:]\n",
    "        positives = j+1\n",
    "        true_positives = np.sum(current_alarms)\n",
    "        false_positives = positives - true_positives\n",
    "        negatives = 3000 - positives\n",
    "        false_negatives = np.sum(current_normals)\n",
    "        true_negatives = negatives - false_negatives\n",
    "        precision = true_positives/(true_positives+false_positives)\n",
    "        recall = true_positives/(true_positives+false_negatives)\n",
    "        f1_score = 2*precision*recall/(precision+recall)\n",
    "        if f1_score > best_score:\n",
    "            best_score = f1_score\n",
    "            precision_on_normal = true_negatives/(true_negatives+false_negatives)\n",
    "            recall_on_normal = true_negatives/(true_negatives+false_positives)\n",
    "            f1_score_on_normal = 2*precision_on_normal*recall_on_normal/(precision_on_normal+recall_on_normal)\n",
    "            macro_f1_score = (f1_score + f1_score_on_normal)/2\n",
    "            best_cut = [j, precision, recall, f1_score, precision_on_normal, recall_on_normal,\n",
    "                        f1_score_on_normal, macro_f1_score]    \n",
    "    print(best_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e440c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get anomaly scores produced on test dataset only for L2H1WL\n",
    "num_head = 1\n",
    "# initialize the model\n",
    "size_in = dataset.get(0).x.shape[1]\n",
    "size_out = int(2 * dataset.get(0).x.shape[1] / num_head)       \n",
    "edge_size = dataset.get(0).edge_attr.shape[1] \n",
    "size_hid1 = int(1.5 * dataset.get(0).x.shape[1] / num_head)\n",
    "model = GATL1noSelf(size_in, size_out, edge_size, num_head, tmax, dropout_prob)\n",
    "model.load_state_dict(torch.load(models_target[0]))\n",
    "# get the result in a dataframe\n",
    "batch_size = 1\n",
    "loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "test_id_list = case_id_list[12000:15000]\n",
    "loss_list = []\n",
    "begin_time = datetime.now()\n",
    "for data_batch in loader:\n",
    "    loss = batch_loss(model.forward(data_batch).float(), data_batch.edge_m_array.float())\n",
    "    if loss > 0:\n",
    "        loss_list.append(loss.item())\n",
    "    else:\n",
    "        loss_list.append(0)      \n",
    "finish_time = datetime.now()    \n",
    "with open(models_txt_target[8], \"a\") as GAT2_file:\n",
    "    print('Anomaly score computation time (training set) for', len(test_dataset), 'cases:', file=GAT2_file)\n",
    "    print(finish_time - begin_time, file=GAT2_file)\n",
    "    GAT2_file.close()\n",
    "GAT2_dictionary = {'case_id': test_id_list, 'Loss': loss_list}\n",
    "loss_df = pd.DataFrame(GAT2_dictionary)\n",
    "loss_df.to_pickle('D:\\Final master thesis evaluation\\exp1\\models\\GATL1h1nl_test_loss.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1690e359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import label dataframe\n",
    "label_csv = pd.read_csv(r'D:\\Final master thesis evaluation\\exp1\\large-0.1-1.csv')\n",
    "label_csv.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "label_csv[\"case_id\"] = pd.to_numeric(label_csv[\"case_id\"])\n",
    "label_csv.case_id.astype(str).astype(int)\n",
    "label_csv.label.astype(str)\n",
    "#label_csv.dtypes\n",
    "test_csv = label_csv.loc[label_csv['case_id'] > 12000]\n",
    "#print(len(validation_csv))\n",
    "label_csv1 = test_csv.loc[test_csv['label']== 'normal']\n",
    "normal_class = len(label_csv1)\n",
    "anom_class = 3000 - len(label_csv1)\n",
    "print('numnber of anomalies in the log:', anom_class)\n",
    "label_csv.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96505096",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df = pd.read_pickle('D:\\Final master thesis evaluation\\exp1\\models\\GATL1h1nl_test_loss.pt')\n",
    "loss_df[\"case_id\"] = pd.to_numeric(loss_df[\"case_id\"])\n",
    "result = pd.merge(loss_df, test_csv, on=[\"case_id\"])\n",
    "sorted_result = result.sort_values(by=['Loss'], ascending = False, ignore_index = True)\n",
    "sorted_list = sorted_result['label'].tolist()\n",
    "predictions = []\n",
    "for j in range (len(sorted_list)):\n",
    "    if sorted_list[j] == 'normal':\n",
    "        predictions.append(0)\n",
    "    else:\n",
    "        predictions.append(1)\n",
    "prediction_array = np.array(predictions)\n",
    "best_score = 0\n",
    "for j in range (len(sorted_list)):\n",
    "    current_alarms = prediction_array[0:j+1]\n",
    "    current_normals = prediction_array[j+1:]\n",
    "    positives = j+1\n",
    "    true_positives = np.sum(current_alarms)\n",
    "    false_positives = positives - true_positives\n",
    "    negatives = 3000 - positives\n",
    "    false_negatives = np.sum(current_normals)\n",
    "    true_negatives = negatives - false_negatives\n",
    "    precision = true_positives/(true_positives+false_positives)\n",
    "    recall = true_positives/(true_positives+false_negatives)\n",
    "    f1_score = 2*precision*recall/(precision+recall)\n",
    "    if f1_score > best_score:\n",
    "        best_score = f1_score\n",
    "        precision_on_normal = true_negatives/(true_negatives+false_negatives)\n",
    "        recall_on_normal = true_negatives/(true_negatives+false_positives)\n",
    "        f1_score_on_normal = 2*precision_on_normal*recall_on_normal/(precision_on_normal+recall_on_normal)\n",
    "        macro_f1_score = (f1_score + f1_score_on_normal)/2\n",
    "        best_cut = [j, precision, recall, f1_score, precision_on_normal, recall_on_normal,\n",
    "                    f1_score_on_normal, macro_f1_score]    \n",
    "print(best_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80951f28",
   "metadata": {},
   "source": [
    "ensemble?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7526eaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_csv = label_csv.loc[label_csv['case_id'] > 9000]\n",
    "validation_csv = validation_csv.loc[label_csv['case_id'] < 12001]\n",
    "label_csv1 = validation_csv.loc[validation_csv['label']== 'normal']\n",
    "normal_class = len(label_csv1)\n",
    "anom_class = 3000 - len(label_csv1)\n",
    "print('numnber of anomalies in the log:', anom_class)\n",
    "loss1_df = pd.read_pickle(loss_df_target[8])\n",
    "loss2_df = pd.read_pickle(loss_df_target[0])\n",
    "loss1_df[\"case_id\"] = pd.to_numeric(loss1_df[\"case_id\"])\n",
    "loss2_df[\"case_id\"] = pd.to_numeric(loss2_df[\"case_id\"])\n",
    "result1 = pd.merge(loss1_df, validation_csv, on=[\"case_id\"])\n",
    "result2 = pd.merge(loss2_df, validation_csv, on=[\"case_id\"])\n",
    "sorted_result1 = result1.sort_values(by=['Loss'], ascending = False, ignore_index = True)\n",
    "sorted_result2 = result2.sort_values(by=['Loss'], ascending = False, ignore_index = True)\n",
    "first_labels = sorted_result1['label'].tolist()\n",
    "second_labels = sorted_result2['label'].tolist()\n",
    "first_labels_limited = first_labels[0:335]\n",
    "second_labels_limited = second_labels[0:335]\n",
    "third_labels_limited = first_labels[0:361]\n",
    "predictions = []\n",
    "for j in range (len(first_labels_limited)):\n",
    "    if first_labels_limited[j] == 'normal':\n",
    "        predictions.append(0)\n",
    "    else:\n",
    "        predictions.append(1)\n",
    "prediction_array = np.array(predictions)\n",
    "print(np.sum(prediction_array), 'anomalies detected')\n",
    "predictions = []\n",
    "for j in range (len(second_labels_limited)):\n",
    "    if second_labels_limited[j] == 'normal':\n",
    "        predictions.append(0)\n",
    "    else:\n",
    "        predictions.append(1)\n",
    "prediction_array = np.array(predictions)\n",
    "print(np.sum(prediction_array), 'anomalies detected')\n",
    "predictions = []\n",
    "for j in range (len(third_labels_limited)):\n",
    "    if third_labels_limited[j] == 'normal':\n",
    "        predictions.append(0)\n",
    "    else:\n",
    "        predictions.append(1)\n",
    "prediction_array = np.array(predictions)\n",
    "print(np.sum(prediction_array), 'anomalies detected')\n",
    "sorted_list1 = sorted_result1['case_id'].tolist()\n",
    "sorted_list2 = sorted_result2['case_id'].tolist()\n",
    "first_sad1 = set(sorted_list1[0:100])\n",
    "first_sad2 = set(sorted_list2[0:100])\n",
    "first_dosad1 = set(sorted_list1[0:200])\n",
    "first_dosad2 = set(sorted_list2[0:200])\n",
    "first_sesad1 = set(sorted_list1[0:300])\n",
    "first_sesad2 = set(sorted_list2[0:300])\n",
    "first_all1 = set(sorted_list1[0:361])\n",
    "first_all2 = set(sorted_list2[0:335])\n",
    "common_sad = first_sad1.intersection(first_sad2)\n",
    "common_dosad = first_dosad1.intersection(first_dosad2)\n",
    "common_sesad = first_sesad1.intersection(first_sesad2)\n",
    "common_all = first_all1.intersection(first_all2)\n",
    "print('Number of common cases in top 100 anomalous scores:', len(common_sad))\n",
    "print('Number of common cases in top 200 anomalous scores:', len(common_dosad))\n",
    "print('Number of common cases in top 300 anomalous scores:', len(common_sesad))\n",
    "print('Number of common cases for the best cut:', len(common_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd62b5b9",
   "metadata": {},
   "source": [
    "Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e443f73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "convergence_list = ['D:\\Final master thesis evaluation\\exp2\\convergence_models\\GATL1h1nl_param.pt',\n",
    "                   'D:\\Final master thesis evaluation\\exp2\\convergence_models\\GATL2h1wl_param.pt']\n",
    "loss_list_target = ['D:\\Final master thesis evaluation\\exp2\\convergence_models\\GATL1h1nl_training_loss.pt',\n",
    "                   'D:\\Final master thesis evaluation\\exp2\\convergence_models\\GATL1h1nl_validation_loss.pt',\n",
    "                   'D:\\Final master thesis evaluation\\exp2\\convergence_models\\GATL2h1wl_training_loss.pt',\n",
    "                   'D:\\Final master thesis evaluation\\exp2\\convergence_models\\GATL2h1wl_validation_loss.pt']\n",
    "first_model_training_loss = []\n",
    "second_model_training_loss = []\n",
    "training_convergence_list = [first_model_training_loss, second_model_training_loss]\n",
    "first_model_validation_loss = []\n",
    "second_model_validation_loss = []\n",
    "validation_convergence_list = [first_model_validation_loss, second_model_validation_loss]\n",
    "number_heads = [1,4,1,4,1,4,1,4,1,4,1,4]\n",
    "model_indices = [0,8]\n",
    "for i in model_indices:\n",
    "    # initialize the model\n",
    "    num_head = number_heads[i]\n",
    "    size_in = dataset.get(0).x.shape[1]\n",
    "    size_out = int(2 * dataset.get(0).x.shape[1] / num_head)       \n",
    "    edge_size = dataset.get(0).edge_attr.shape[1]    \n",
    "    if i == 0:\n",
    "        model = GATL1noSelf(size_in, size_out, edge_size, num_head, tmax, dropout_prob)\n",
    "    elif i == 8:\n",
    "        size_hid1 = int(1.5 * dataset.get(0).x.shape[1] / num_head)\n",
    "        model = GATL2withSelf(size_in, size_out, size_hid1, edge_size, num_head, tmax, dropout_prob)\n",
    "    # setting the minibatch size\n",
    "    batch_size = 64\n",
    "    loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "    # move to GPU (if available)\n",
    "    model = model.to(device)\n",
    "    # inizialize the optimizer\n",
    "    lr = 0.01\n",
    "    weight_decay = 5e-4\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    # training of the model\n",
    "    epochs = 1\n",
    "    train_hist = {}\n",
    "    train_hist['loss'] = []\n",
    "    # Initialize training\n",
    "    for layer in model.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            layer.reset_parameters()\n",
    "    model.train()\n",
    "    # training loop\n",
    "    for epoch in range(epochs):\n",
    "        for idx, data_batch in enumerate(loader):\n",
    "            data_batch = data_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = batch_loss(model.forward(data_batch).float(), data_batch.edge_m_array.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_hist['loss'].append(loss.item())\n",
    "    # save and load learned parameters\n",
    "    if i == 0:\n",
    "        torch.save(model.state_dict(), convergence_list[0])\n",
    "    else:\n",
    "        torch.save(model.state_dict(), convergence_list[1])\n",
    "    batch_size = 1\n",
    "    loss_list = []\n",
    "    for data_batch in loader:\n",
    "        loss = batch_loss(model.forward(data_batch).float(), data_batch.edge_m_array.float())\n",
    "        if loss > 0:\n",
    "            loss_list.append(loss.item())\n",
    "        else:\n",
    "            loss_list.append(0)\n",
    "    if i == 0:\n",
    "        training_convergence_list [0].append(sum(loss_list)/9000)\n",
    "    else:\n",
    "        training_convergence_list [1].append(sum(loss_list)/9000)\n",
    "    loader = DataLoader(validation_dataset, batch_size=batch_size)\n",
    "    loss_list = []\n",
    "    for data_batch in loader:\n",
    "        loss = batch_loss(model.forward(data_batch).float(), data_batch.edge_m_array.float())\n",
    "        if loss > 0:\n",
    "            loss_list.append(loss.item())\n",
    "        else:\n",
    "            loss_list.append(0)\n",
    "    if i == 0:\n",
    "        validation_convergence_list [0].append(sum(loss_list)/3000)\n",
    "    else:\n",
    "        validation_convergence_list [1].append(sum(loss_list)/3000)      \n",
    "\n",
    "for training_epochs in range (100):\n",
    "    print(training_epochs)\n",
    "    for i in model_indices:\n",
    "        # initialize the model    \n",
    "        num_head = number_heads[i]\n",
    "        size_in = dataset.get(0).x.shape[1]\n",
    "        size_out = int(2 * dataset.get(0).x.shape[1] / num_head)       \n",
    "        edge_size = dataset.get(0).edge_attr.shape[1]    \n",
    "        if i == 0:\n",
    "            model = GATL1noSelf(size_in, size_out, edge_size, num_head, tmax, dropout_prob)\n",
    "            model.load_state_dict(torch.load(convergence_list[0]))\n",
    "        elif i == 8:\n",
    "            size_hid1 = int(1.5 * dataset.get(0).x.shape[1] / num_head)\n",
    "            model = GATL2withSelf(size_in, size_out, size_hid1, edge_size, num_head, tmax, dropout_prob)\n",
    "            model.load_state_dict(torch.load(convergence_list[1]))        \n",
    "        # setting the minibatch size\n",
    "        batch_size = 64\n",
    "        loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "        # move to GPU (if available)\n",
    "        model = model.to(device)\n",
    "        # inizialize the optimizer\n",
    "        lr = 0.01\n",
    "        weight_decay = 5e-4\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        # training of the model\n",
    "        epochs = 1\n",
    "        train_hist = {}\n",
    "        train_hist['loss'] = []\n",
    "        model.train()\n",
    "        # training loop\n",
    "        for epoch in range(epochs):\n",
    "            for idx, data_batch in enumerate(loader):\n",
    "                data_batch = data_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                loss = batch_loss(model.forward(data_batch).float(), data_batch.edge_m_array.float())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_hist['loss'].append(loss.item())\n",
    "        # save and load learned parameters\n",
    "        if i == 0:\n",
    "            torch.save(model.state_dict(), convergence_list[0])\n",
    "        else:\n",
    "            torch.save(model.state_dict(), convergence_list[1])\n",
    "        batch_size = 1\n",
    "        loss_list = []\n",
    "        for data_batch in loader:\n",
    "            loss = batch_loss(model.forward(data_batch).float(), data_batch.edge_m_array.float())\n",
    "            if loss > 0:\n",
    "                loss_list.append(loss.item())\n",
    "            else:\n",
    "                loss_list.append(0)\n",
    "        if i == 0:\n",
    "            training_convergence_list [0].append(sum(loss_list)/9000)\n",
    "        else:\n",
    "            training_convergence_list [1].append(sum(loss_list)/9000)\n",
    "        loader = DataLoader(validation_dataset, batch_size=batch_size)\n",
    "        loss_list = []\n",
    "        for data_batch in loader:\n",
    "            loss = batch_loss(model.forward(data_batch).float(), data_batch.edge_m_array.float())\n",
    "            if loss > 0:\n",
    "                loss_list.append(loss.item())\n",
    "            else:\n",
    "                loss_list.append(0)\n",
    "        if i == 0:\n",
    "            validation_convergence_list [0].append(sum(loss_list)/3000)\n",
    "        else:\n",
    "            validation_convergence_list [1].append(sum(loss_list)/3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7eff037",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_file = open(loss_list_target[0], \"wb\")\n",
    "pickle.dump(training_convergence_list [0], loss_file)\n",
    "loss_file.close()\n",
    "loss_file = open(loss_list_target[2], \"wb\")\n",
    "pickle.dump(training_convergence_list [1], loss_file)\n",
    "loss_file.close() \n",
    "loss_file = open(loss_list_target[1], \"wb\")\n",
    "pickle.dump(validation_convergence_list [0], loss_file)\n",
    "loss_file.close() \n",
    "loss_file = open(loss_list_target[3], \"wb\")\n",
    "pickle.dump(validation_convergence_list [1], loss_file)\n",
    "loss_file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e6dec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_convergence1 = np.array(training_convergence_list [0])*64\n",
    "validation_convergence1 = np.array(validation_convergence_list [0])\n",
    "training_convergence2 = np.array(training_convergence_list [1])*64\n",
    "validation_convergence2 = np.array(validation_convergence_list [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b763b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = np.arange(1,93)\n",
    "figure(figsize=(24, 18), dpi=300)\n",
    "nextplot()\n",
    "plt.title(\"Training, and validation loss: GATL1H1NL model\")\n",
    "plt.plot(x_values, training_convergence1, color =\"blue\", label = 'Average batch training loss')\n",
    "plt.plot(x_values, validation_convergence1, color =\"Orange\", label = 'Average Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3cc112",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = np.arange(1,92)\n",
    "figure(figsize=(24, 18), dpi=300)\n",
    "nextplot()\n",
    "plt.title(\"Training, and validation loss: GATL2h1WL model\")\n",
    "plt.plot(x_values, training_convergence2, color =\"blue\", label = 'Average batch training loss')\n",
    "plt.plot(x_values, validation_convergence2, color =\"Orange\", label = 'Average Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02862d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
