{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6bb0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os.path as osp\n",
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import random\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "from pm4py.objects.log.exporter.xes import exporter as xes_exporter\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from pm4py.algo.filtering.log.attributes import attributes_filter\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Dataset, Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import NNConv\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069681a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkinbaronDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super(SkinbaronDataset, self).__init__(root, transform, pre_transform, pre_filter)\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return processed_graphs\n",
    "    def len(self):\n",
    "        return len(self.processed_file_names)\n",
    "    def get(self, idx):\n",
    "        data = torch.load(osp.join(self.processed_dir, f'data_{idx}.pt'))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be049a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define GCN (ECC), and GAT neural network architectures\n",
    "class ECCCONV(nn.Module):\n",
    "    \"\"\"Define ECCCONV class for the Edge-Conditioned Convolutional (ECC) neural network.\"\"\"\n",
    "\n",
    "    def __init__(self, size_in, size_out, edge_size, tmax):\n",
    "        super(ECCCONV, self).__init__()\n",
    "        nn1 = nn.Linear(edge_size, size_in * size_out)\n",
    "        self.conv1 = NNConv(size_in, size_out, nn1)\n",
    "        self.lin1 = nn.Linear(size_in, size_out, bias=False)\n",
    "        self.readout = nn.Linear(2 * size_out, tmax - 1)\n",
    "\n",
    "    def forward(self, graph):\n",
    "        graph.x = F.elu(self.conv1(graph.x, graph.edge_index, graph.edge_attr) + self.lin1(graph.x))\n",
    "        s_t_nodes = torch.cat((graph.x[int(graph.edge_index[0, 0])], graph.x[int(graph.edge_index[1, 0])]))\n",
    "        m_array_pred = torch.sigmoid(self.readout(s_t_nodes))\n",
    "        for i in range(1, graph.edge_index.shape[1]):  # iterate over the number of edges in the batch (called graph)\n",
    "            s_t_nodes = torch.cat((graph.x[int(graph.edge_index[0, i])], graph.x[int(graph.edge_index[1, i])]))\n",
    "            edge_label_pred = torch.sigmoid(self.readout(s_t_nodes))\n",
    "            m_array_pred = torch.cat((m_array_pred, edge_label_pred), 0)\n",
    "        m_array_pred = m_array_pred.view(-1, tmax - 1)\n",
    "        return m_array_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ccbd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "targat_path = r'D:\\Final master thesis evaluation\\small-fifteen\\data'\n",
    "processed_Pattern = r\"D:\\Final master thesis evaluation\\small-fifteen\\data\\processed\\*.pt\"\n",
    "case_id_target = r'D:\\Final master thesis evaluation\\small-fifteen\\case_id_list.pkl'\n",
    "models_target = r'D:\\Final master thesis evaluation\\small-fifteen\\ECC_param.pt'\n",
    "loss_df_target = r'D:\\Final master thesis evaluation\\small-fifteen\\ECC_loss.pt'\n",
    "# set the device to GPU or CPU (only if GPU is not available we will use CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# for reproducibility of the result (note: we use SGD for optimization)\n",
    "np.random.seed(0)\n",
    "# Define binary cross entropy loss.\n",
    "batch_loss = nn.BCELoss()\n",
    "# creating dataset\n",
    "processed_graphs = glob.glob(processed_Pattern)\n",
    "dataset = SkinbaronDataset(root=targat_path)\n",
    "# train/validation/test split\n",
    "train_dataset = dataset[0:9000]\n",
    "test_dataset = dataset[12000:15000]\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')\n",
    "# Retrieving the case_id list from saved file on the disk:\n",
    "case_id_file = open(case_id_target, \"rb\")\n",
    "case_id_list = pickle.load(case_id_file)\n",
    "case_id_file.close()\n",
    "# find the length of the longest trace based on the edge_m_array attribute\n",
    "tmax = int(dataset.get(0).edge_m_array.shape[1]) + 1\n",
    "dropout_prob = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261623c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize ECC model\n",
    "num_head = 1\n",
    "size_in = dataset.get(0).x.shape[1]\n",
    "size_out = int(2 * dataset.get(0).x.shape[1] / num_head)\n",
    "edge_size = dataset.get(0).edge_attr.shape[1]\n",
    "model = ECCCONV(size_in, size_out, edge_size, tmax)\n",
    "# setting the minibatch size\n",
    "batch_size = 8\n",
    "loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "# move to GPU (if available)\n",
    "model = model.to(device)\n",
    "# inizialize the optimizer\n",
    "lr = 0.001\n",
    "weight_decay = 0\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "# training of the model\n",
    "epochs = 100\n",
    "train_hist = {}\n",
    "train_hist['loss'] = []\n",
    "# Initialize training\n",
    "for layer in model.children():\n",
    "    if hasattr(layer, 'reset_parameters'):\n",
    "        layer.reset_parameters()\n",
    "model.train()\n",
    "# training loop\n",
    "for epoch in range(epochs):\n",
    "    print(epoch)\n",
    "    for idx, data_batch in enumerate(loader):\n",
    "        data_batch = data_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = batch_loss(model.forward(data_batch).float(), data_batch.edge_m_array.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_hist['loss'].append(loss.item())\n",
    "        # print('[Epoch %4d/%4d] [Batch %4d/%4d] Loss: % 2.2e' % (epoch + 1, epochs,idx + 1, len(loader),loss.item()))\n",
    "# save and load learned parameters\n",
    "torch.save(model.state_dict(), models_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5465e39f",
   "metadata": {},
   "outputs": [],
   "source": [
    " get anomaly scores produced on test dataset only for the best hyperparameter configuration\n",
    "test_loss_path = r'D:\\Final master thesis evaluation\\small-fifteen\\ECC_loss.pt'\n",
    "label_csv = pd.read_csv(r'D:\\Final master thesis evaluation\\small-fifteen\\small-0.1-1.csv')\n",
    "label_csv.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "label_csv[\"case_id\"] = pd.to_numeric(label_csv[\"case_id\"])\n",
    "label_csv.case_id.astype(str).astype(int)\n",
    "label_csv.label.astype(str)\n",
    "test_csv = label_csv.loc[label_csv['case_id'] > 12000]\n",
    "# get the result in a dataframe\n",
    "batch_size = 1\n",
    "loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "test_id_list = case_id_list[12000:15000]\n",
    "loss_list = []\n",
    "for data_batch in loader:\n",
    "    loss = batch_loss(model.forward(data_batch).float(), data_batch.edge_m_array.float())\n",
    "    if loss > 0:\n",
    "        loss_list.append(loss.item())\n",
    "    else:\n",
    "        loss_list.append(0)  \n",
    "GAT2_dictionary = {'case_id': test_id_list, 'Loss': loss_list}\n",
    "loss_df = pd.DataFrame(GAT2_dictionary)\n",
    "loss_df.to_pickle(test_loss_path)\n",
    "loss_df[\"case_id\"] = pd.to_numeric(loss_df[\"case_id\"])\n",
    "result = pd.merge(loss_df, test_csv, on=[\"case_id\"])\n",
    "sorted_result = result.sort_values(by=['Loss'], ascending = False, ignore_index = True)\n",
    "sorted_list = sorted_result['label'].tolist()\n",
    "predictions = []\n",
    "for j in range (len(sorted_list)):\n",
    "    if sorted_list[j] == 'normal':\n",
    "        predictions.append(0)\n",
    "    else:\n",
    "        predictions.append(1)\n",
    "prediction_array = np.array(predictions)\n",
    "best_score = 0\n",
    "for j in range (len(sorted_list)):\n",
    "    current_alarms = prediction_array[0:j+1]\n",
    "    current_normals = prediction_array[j+1:]\n",
    "    positives = j+1\n",
    "    true_positives = np.sum(current_alarms)\n",
    "    false_positives = positives - true_positives\n",
    "    negatives = 3000 - positives\n",
    "    false_negatives = np.sum(current_normals)\n",
    "    true_negatives = negatives - false_negatives\n",
    "    precision = true_positives/(true_positives+false_positives)\n",
    "    recall = true_positives/(true_positives+false_negatives)\n",
    "    f1_score = 2*precision*recall/(precision+recall)\n",
    "    if f1_score > best_score:\n",
    "        best_score = f1_score\n",
    "        precision_on_normal = true_negatives/(true_negatives+false_negatives)\n",
    "        recall_on_normal = true_negatives/(true_negatives+false_positives)\n",
    "        f1_score_on_normal = 2*precision_on_normal*recall_on_normal/(precision_on_normal+recall_on_normal)\n",
    "        macro_f1_score = (f1_score + f1_score_on_normal)/2\n",
    "        best_result = [precision, recall, f1_score, precision_on_normal, recall_on_normal,\n",
    "                    f1_score_on_normal, macro_f1_score]\n",
    "        best_cut = [j, false_positives, false_negatives]\n",
    "print(best_result)\n",
    "print(best_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f153a908",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
